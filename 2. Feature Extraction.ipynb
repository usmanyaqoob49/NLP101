{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224ea5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b17906",
   "metadata": {},
   "source": [
    "# Loading the PreProcessed Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c577a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>simle_words_tokens</th>\n",
       "      <th>nltk_word_tokenize</th>\n",
       "      <th>nltk_sentence_tokenize</th>\n",
       "      <th>porter_stemming</th>\n",
       "      <th>lemmitized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>one    reviewers  mentioned   watching  1 oz e...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['one', 'reviewers', 'mentioned', 'watching', ...</td>\n",
       "      <td>['one', 'reviewers', 'mentioned', 'watching', ...</td>\n",
       "      <td>['one    reviewers  mentioned   watching  1 oz...</td>\n",
       "      <td>one review mention watch 1 oz episod youll hoo...</td>\n",
       "      <td>one reviewer mentioned watching 1 oz episode y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wonderful little production  filming techniqu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['wonderful', 'little', 'production', 'filming...</td>\n",
       "      <td>['wonderful', 'little', 'production', 'filming...</td>\n",
       "      <td>[' wonderful little production  filming techni...</td>\n",
       "      <td>wonder littl product film techniqu unassum old...</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>thought    wonderful way  spend time    hot s...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['thought', 'wonderful', 'way', 'spend', 'time...</td>\n",
       "      <td>['thought', 'wonderful', 'way', 'spend', 'time...</td>\n",
       "      <td>[' thought    wonderful way  spend time    hot...</td>\n",
       "      <td>thought wonder way spend time hot summer weeke...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>basically theres  family   little boy jake thi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['basically', 'theres', 'family', 'little', 'b...</td>\n",
       "      <td>['basically', 'theres', 'family', 'little', 'b...</td>\n",
       "      <td>['basically theres  family   little boy jake t...</td>\n",
       "      <td>basic there famili littl boy jake think there ...</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>petter matteis love   time  money   visually s...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['petter', 'matteis', 'love', 'time', 'money',...</td>\n",
       "      <td>['petter', 'matteis', 'love', 'time', 'money',...</td>\n",
       "      <td>['petter matteis love   time  money   visually...</td>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review sentiment  \\\n",
       "0           0  one    reviewers  mentioned   watching  1 oz e...  positive   \n",
       "1           1   wonderful little production  filming techniqu...  positive   \n",
       "2           2   thought    wonderful way  spend time    hot s...  positive   \n",
       "3           3  basically theres  family   little boy jake thi...  negative   \n",
       "4           4  petter matteis love   time  money   visually s...  positive   \n",
       "\n",
       "                                  simle_words_tokens  \\\n",
       "0  ['one', 'reviewers', 'mentioned', 'watching', ...   \n",
       "1  ['wonderful', 'little', 'production', 'filming...   \n",
       "2  ['thought', 'wonderful', 'way', 'spend', 'time...   \n",
       "3  ['basically', 'theres', 'family', 'little', 'b...   \n",
       "4  ['petter', 'matteis', 'love', 'time', 'money',...   \n",
       "\n",
       "                                  nltk_word_tokenize  \\\n",
       "0  ['one', 'reviewers', 'mentioned', 'watching', ...   \n",
       "1  ['wonderful', 'little', 'production', 'filming...   \n",
       "2  ['thought', 'wonderful', 'way', 'spend', 'time...   \n",
       "3  ['basically', 'theres', 'family', 'little', 'b...   \n",
       "4  ['petter', 'matteis', 'love', 'time', 'money',...   \n",
       "\n",
       "                              nltk_sentence_tokenize  \\\n",
       "0  ['one    reviewers  mentioned   watching  1 oz...   \n",
       "1  [' wonderful little production  filming techni...   \n",
       "2  [' thought    wonderful way  spend time    hot...   \n",
       "3  ['basically theres  family   little boy jake t...   \n",
       "4  ['petter matteis love   time  money   visually...   \n",
       "\n",
       "                                     porter_stemming  \\\n",
       "0  one review mention watch 1 oz episod youll hoo...   \n",
       "1  wonder littl product film techniqu unassum old...   \n",
       "2  thought wonder way spend time hot summer weeke...   \n",
       "3  basic there famili littl boy jake think there ...   \n",
       "4  petter mattei love time money visual stun film...   \n",
       "\n",
       "                                          lemmitized  \n",
       "0  one reviewer mentioned watching 1 oz episode y...  \n",
       "1  wonderful little production filming technique ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically there family little boy jake think t...  \n",
       "4  petter matteis love time money visually stunni...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('pre_processed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c29b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnamed col\n",
    "df.drop('Unnamed: 0', axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9174362e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>simle_words_tokens</th>\n",
       "      <th>nltk_word_tokenize</th>\n",
       "      <th>nltk_sentence_tokenize</th>\n",
       "      <th>porter_stemming</th>\n",
       "      <th>lemmitized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one    reviewers  mentioned   watching  1 oz e...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['one', 'reviewers', 'mentioned', 'watching', ...</td>\n",
       "      <td>['one', 'reviewers', 'mentioned', 'watching', ...</td>\n",
       "      <td>['one    reviewers  mentioned   watching  1 oz...</td>\n",
       "      <td>one review mention watch 1 oz episod youll hoo...</td>\n",
       "      <td>one reviewer mentioned watching 1 oz episode y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production  filming techniqu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['wonderful', 'little', 'production', 'filming...</td>\n",
       "      <td>['wonderful', 'little', 'production', 'filming...</td>\n",
       "      <td>[' wonderful little production  filming techni...</td>\n",
       "      <td>wonder littl product film techniqu unassum old...</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought    wonderful way  spend time    hot s...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['thought', 'wonderful', 'way', 'spend', 'time...</td>\n",
       "      <td>['thought', 'wonderful', 'way', 'spend', 'time...</td>\n",
       "      <td>[' thought    wonderful way  spend time    hot...</td>\n",
       "      <td>thought wonder way spend time hot summer weeke...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres  family   little boy jake thi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['basically', 'theres', 'family', 'little', 'b...</td>\n",
       "      <td>['basically', 'theres', 'family', 'little', 'b...</td>\n",
       "      <td>['basically theres  family   little boy jake t...</td>\n",
       "      <td>basic there famili littl boy jake think there ...</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love   time  money   visually s...</td>\n",
       "      <td>positive</td>\n",
       "      <td>['petter', 'matteis', 'love', 'time', 'money',...</td>\n",
       "      <td>['petter', 'matteis', 'love', 'time', 'money',...</td>\n",
       "      <td>['petter matteis love   time  money   visually...</td>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  one    reviewers  mentioned   watching  1 oz e...  positive   \n",
       "1   wonderful little production  filming techniqu...  positive   \n",
       "2   thought    wonderful way  spend time    hot s...  positive   \n",
       "3  basically theres  family   little boy jake thi...  negative   \n",
       "4  petter matteis love   time  money   visually s...  positive   \n",
       "\n",
       "                                  simle_words_tokens  \\\n",
       "0  ['one', 'reviewers', 'mentioned', 'watching', ...   \n",
       "1  ['wonderful', 'little', 'production', 'filming...   \n",
       "2  ['thought', 'wonderful', 'way', 'spend', 'time...   \n",
       "3  ['basically', 'theres', 'family', 'little', 'b...   \n",
       "4  ['petter', 'matteis', 'love', 'time', 'money',...   \n",
       "\n",
       "                                  nltk_word_tokenize  \\\n",
       "0  ['one', 'reviewers', 'mentioned', 'watching', ...   \n",
       "1  ['wonderful', 'little', 'production', 'filming...   \n",
       "2  ['thought', 'wonderful', 'way', 'spend', 'time...   \n",
       "3  ['basically', 'theres', 'family', 'little', 'b...   \n",
       "4  ['petter', 'matteis', 'love', 'time', 'money',...   \n",
       "\n",
       "                              nltk_sentence_tokenize  \\\n",
       "0  ['one    reviewers  mentioned   watching  1 oz...   \n",
       "1  [' wonderful little production  filming techni...   \n",
       "2  [' thought    wonderful way  spend time    hot...   \n",
       "3  ['basically theres  family   little boy jake t...   \n",
       "4  ['petter matteis love   time  money   visually...   \n",
       "\n",
       "                                     porter_stemming  \\\n",
       "0  one review mention watch 1 oz episod youll hoo...   \n",
       "1  wonder littl product film techniqu unassum old...   \n",
       "2  thought wonder way spend time hot summer weeke...   \n",
       "3  basic there famili littl boy jake think there ...   \n",
       "4  petter mattei love time money visual stun film...   \n",
       "\n",
       "                                          lemmitized  \n",
       "0  one reviewer mentioned watching 1 oz episode y...  \n",
       "1  wonderful little production filming technique ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically there family little boy jake think t...  \n",
       "4  petter matteis love time money visually stunni...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52783b33",
   "metadata": {},
   "source": [
    "## 1. Using Bag of Words for Feature Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e4dcf",
   "metadata": {},
   "source": [
    "Bag of words is basically about counting the number of time each unique word is repeated in the each Text (Row).\n",
    "\n",
    "        Like for text:\n",
    "                Hello World and Hello There -> Hello = 2, World = 2, There = 1\n",
    "                \n",
    "                And, of and these types are words are not considered using Bag of Words Technique.\n",
    "        \n",
    "And according to Bag of words similar class text data will have same number of frequency for each word in each document/row of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13083bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words can be implemented by using scikit learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv= CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170790b",
   "metadata": {},
   "source": [
    "There is hyperparameter of the cv, 'binary' which is by default false. But when it is true basically it does not count the frequency of the vocabulary words but it just check if the word exist or not. Means make the frequency in binary 0, 1.\n",
    "\n",
    "For Sentiment Analysis Projects it is proven by research to make this True, means just check occurance of word, for rest of project it is used as False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efbe902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will use bag of words technique to extract features from text\n",
    "bag_of_words= cv.fit_transform(df['lemmitized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a2037",
   "metadata": {},
   "source": [
    "Vocabulary is basically all unique words in the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78e57392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c33b6a4-ec48-45a7-907b-b77e5419a680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  210113\n"
     ]
    }
   ],
   "source": [
    "print(\"Length: \", len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b62d0",
   "metadata": {},
   "source": [
    "First 10 Vocabulary Words and there index or positioning in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2149242e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 132541,\n",
       " 'reviewer': 154846,\n",
       " 'mentioned': 117204,\n",
       " 'watching': 201213,\n",
       " 'oz': 136158,\n",
       " 'episode': 61265,\n",
       " 'youll': 208834,\n",
       " 'hooked': 88929,\n",
       " 'right': 155571,\n",
       " 'exactly': 63150}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools \n",
    "N = 10\n",
    "out = dict(itertools.islice(cv.vocabulary_.items(), N)) \n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dc491f",
   "metadata": {},
   "source": [
    "If we convert sample rows into array and check that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed56323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22edb19d-f3ed-48e5-9353-993a43ff8229",
   "metadata": {},
   "source": [
    "**Main Problem with Bag of Words is that if we two sentences:**\n",
    "\n",
    "        1. This is a good Movie.\n",
    "        2. This is not a good Movie.\n",
    "**Here in both snetences only difference is of not and frequency of the words is also same so according to Bag of Words Technique these both vectors will lie very close to each othet but in reality there are opposite**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f69a3b",
   "metadata": {},
   "source": [
    "### Bag of Ngrams For Feature Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c2e3ce-b760-4aea-b438-487dd81baf72",
   "metadata": {},
   "source": [
    "So basically in Bag of words we were considering vocabualry of single unique word, that causes the problem of ordering.\n",
    "\n",
    "Ngram technique is very similar to bag of words but the main difference comes in considering the number of words in vocaulary. If we have bi-gram then we will consider two unique words combination as vocabulary entery, for tri-gram we will consider three unique words combination and similarly till n-grams.\n",
    "\n",
    "For Example:\n",
    "\n",
    "        Hello world Hi\n",
    "        For Bi-Gram ----> Vocab: Hello world | world Hi\n",
    "\n",
    "We can go for more than tri gram because we will get error as it has max 3 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa866f8-3b8d-4992-82f1-2bacb5f8c7c4",
   "metadata": {},
   "source": [
    "**We will use same CountVectorizer from sklearn but we have a hyperparameter that helps us to set the value for the n-gram which is 'ngram_range' by default it is (1,1) representing uni gram or simple Bag of Words Technique**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97218c-5566-4b6c-9a4b-84b5acd489ae",
   "metadata": {},
   "source": [
    "        For Bi-Gram: ngram_range(2,2)\n",
    "        For Tri-Gram: ngram_range(3,3)\n",
    "        and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae2fdbc-d3e3-455c-abc0-2e65214b4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_cv= CountVectorizer(binary=True, ngram_range=(2,2))\n",
    "bigram= bigram_cv.fit_transform(df['lemmitized'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "094699e3-6dcd-491f-ad7c-4436945ceb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one reviewer': 1936055,\n",
       " 'reviewer mentioned': 2281828,\n",
       " 'mentioned watching': 1733187,\n",
       " 'watching oz': 2983159,\n",
       " 'oz episode': 1972596,\n",
       " 'episode youll': 877927,\n",
       " 'youll hooked': 3101002,\n",
       " 'hooked right': 1308914,\n",
       " 'right exactly': 2291748,\n",
       " 'exactly happened': 913895}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10\n",
    "out = dict(itertools.islice(bigram_cv.vocabulary_.items(), N)) \n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0fbcbd-45b1-418f-b344-13e9438db397",
   "metadata": {},
   "source": [
    "**For ngram_range(1,2) our vocabulary will have unigrams and bigrams both**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa9757-3992-48c1-9a11-b5a397c23e05",
   "metadata": {},
   "source": [
    "Main problem that ngrams solve is that:\n",
    "\n",
    "    1. This movie is very good.\n",
    "    2. This movie is not good.\n",
    "\n",
    "For these sentences in simple bag of words there was only one difference of not and very but for Bi gram:\n",
    "\n",
    "        This is | movie is | is very | very good | is not | not good\n",
    "          1     |  1       |  1      |    1      |  0     |  0\n",
    "          1     |  1       |  0      |    0      |  1     |  1\n",
    "\n",
    "**We have difference in 4 places so now they are pretty distant. Now we are better able to catch the semantic meansing and for the larger sentence it becomes more prominent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c17001-c847-4c05-8359-ed76d6c13b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary with Uni-gram:  210113\n",
      "Length of Vocabulary with Bi-gram:  3111457\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Vocabulary with Uni-gram: \", len(cv.vocabulary_))\n",
    "print('Length of Vocabulary with Bi-gram: ', len(bigram_cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542c2b0-845e-4100-a0ce-9c3c01183979",
   "metadata": {},
   "source": [
    "Main Problem is that our vocabulary length is increasing that makes the overall algorithm or model slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba1ab7-4720-4a65-9b5d-5eb7c39e0779",
   "metadata": {},
   "source": [
    "## 3: TF-IDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089f4c1-2e2b-4fa7-ac0e-f03ebbded626",
   "metadata": {},
   "source": [
    "Till now we have seen vectorizer that give numbers to words according to there presence or absence but TF-IDF which stands for **Term Frequency-Inverse Document Frequecny** assign weight to each unique/vocabulary word in the document.\n",
    "\n",
    "\n",
    "Theoratical Logic:\n",
    "Weight is assign on the basis of the thing that if word is common in a single document/row/sentence and it is not common in rest of data points/documents/rows then it will have higher weight in that document.\n",
    "On the other hand if the word is common in the complete document then it will be assign lower weight for that term in specific document.\n",
    "\n",
    "Mathematical Logic:\n",
    "It assigns weights according or on the basis of two things:\n",
    "\n",
    "        1. TF-Term Frequency: Term Frequency tell us how much that word is repeated in that row/document/sentence.\n",
    "                TF (t,d) = Number of time word(t) appears in document(d) / Total numbers of words in document(d)\n",
    "\n",
    "                Basically by this we get idea if that term or word is rare or common in that sentence/document/data point.\n",
    "\n",
    "        2. IDF-Inverse Document Frequency: It tell us how much common or rare that term is in all the documents.\n",
    "                IDF (t) = loge [(Total number of documents/rows in corpus/complete dataset) / (Number of documents with that                                                             specific term t)]\n",
    "\n",
    "\n",
    "        In the end we multiply TF and IDF to get the Weight for the specific term:\n",
    "        \n",
    "                -If term is very common and it is in almost all rows/documents then denuminator will be high value or in short                                loge(1) = 0, so IDF of common word will be 0. (In implementation of sklearn + 1 is added with whole idf to not ignore the common term)So when multiplied with TF even if its high it will assign low weight.\n",
    "\n",
    "                -If term is rare in the whole dataset so that means it has weightage for that doc so denuminator will become low andlog value will be higher and after multiplying with TF overall weight that will be assigned will be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b646c00-ca64-4bfa-968c-e35ba83c673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4df199c-16a5-4e71-b11f-b569ecd886b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf= TfidfVectorizer()\n",
    "tfidf_features= tfidf.fit_transform(df['lemmitized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17bb0de3-7f05-4c8f-b75d-257adbb4cace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x210113 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4870866 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161b6d5-d6d0-4e83-b8cd-5d79a25bdd33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
